{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1176415,"sourceType":"datasetVersion","datasetId":667889}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Mask Detection. Faster R-SNN vs. YOLOv8","metadata":{}},{"cell_type":"markdown","source":"##### he notebook presents an instance of object detection problem solution using the two popular models mentioned in the title. The last one is a pretty newborn version as of this date (June 2023).\n\n\n##### The project is based on the Kaggle Face Mask Detection dataset. Three classes are presented: with mask, without mask, mask weared incorrectly.","metadata":{}},{"cell_type":"markdown","source":"## Importing required libraries","metadata":{}},{"cell_type":"code","source":"# !python --version","metadata":{"execution":{"iopub.status.busy":"2024-06-22T11:17:05.573931Z","iopub.execute_input":"2024-06-22T11:17:05.574288Z","iopub.status.idle":"2024-06-22T11:17:05.579259Z","shell.execute_reply.started":"2024-06-22T11:17:05.574257Z","shell.execute_reply":"2024-06-22T11:17:05.578324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install cython","metadata":{"execution":{"iopub.status.busy":"2024-06-22T11:17:05.594467Z","iopub.execute_input":"2024-06-22T11:17:05.594753Z","iopub.status.idle":"2024-06-22T11:17:18.834629Z","shell.execute_reply.started":"2024-06-22T11:17:05.594730Z","shell.execute_reply":"2024-06-22T11:17:18.833620Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install pycocotools\n!pip install faster-coco-eval\n!pip install torchmetrics\n!pip install torchmetrics[detection]\n!pip install torchvision>=0.8","metadata":{"execution":{"iopub.status.busy":"2024-06-22T11:17:18.836695Z","iopub.execute_input":"2024-06-22T11:17:18.837028Z","iopub.status.idle":"2024-06-22T11:18:21.860434Z","shell.execute_reply.started":"2024-06-22T11:17:18.836998Z","shell.execute_reply":"2024-06-22T11:18:21.859398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install mean_average_precision # To read the MeanAveragePrecision correctly","metadata":{"execution":{"iopub.status.busy":"2024-06-22T11:18:21.861771Z","iopub.execute_input":"2024-06-22T11:18:21.862074Z","iopub.status.idle":"2024-06-22T11:18:34.346741Z","shell.execute_reply.started":"2024-06-22T11:18:21.862047Z","shell.execute_reply":"2024-06-22T11:18:34.345803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install --upgrade git+https://github.com/bes-dev/mean_average_precision.git","metadata":{"execution":{"iopub.status.busy":"2024-06-22T11:18:34.348705Z","iopub.execute_input":"2024-06-22T11:18:34.349020Z","iopub.status.idle":"2024-06-22T11:18:48.480262Z","shell.execute_reply.started":"2024-06-22T11:18:34.348991Z","shell.execute_reply":"2024-06-22T11:18:48.479075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip show pycocotools\n!pip show faster-coco-eval\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\nimport os\nimport shutil\nimport time\nimport xml.etree.ElementTree as et\nfrom sklearn.model_selection import train_test_split\n\nimport torch\nimport torchvision\nimport torchmetrics\nfrom torchvision import transforms, datasets, models\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n#from torchmetrics.detection.mean_ap import MeanAveragePrecision\nfrom torchmetrics.detection import MeanAveragePrecision\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\nimport cv2\n\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\n!pip install ultralytics\nfrom ultralytics import YOLO, settings","metadata":{"execution":{"iopub.status.busy":"2024-06-22T11:18:48.483287Z","iopub.execute_input":"2024-06-22T11:18:48.483635Z","iopub.status.idle":"2024-06-22T11:19:34.256769Z","shell.execute_reply.started":"2024-06-22T11:18:48.483603Z","shell.execute_reply":"2024-06-22T11:19:34.255847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Faster R-CNN ResNet50 FPN v2","metadata":{}},{"cell_type":"markdown","source":"For our project, we will choose the FASTERRCNN_RESET 50_FPS_V2 model, since it is more perfect for our datasets","metadata":{}},{"cell_type":"markdown","source":"# Data Readout","metadata":{}},{"cell_type":"code","source":"# file path\nimages_dir = '/kaggle/input/face-mask-detection/images/'\nannotations_dir = '/kaggle/input/face-mask-detection/annotations/'\n\n\nclass FaceMaskDataset(torch.utils.data.Dataset):\n\n    def __init__(self, images_dir, annotation_dir, width, height, transforms=None):\n        self.transforms = transforms\n        self.images_dir = images_dir\n        self.annotation_dir = annotation_dir\n        self.height = height\n        self.width = width\n        \n        # sorting images\n        self.imgs = [image for image in sorted(os.listdir(images_dir))]\n        self.annotate = [image for image in sorted(os.listdir(annotation_dir))]\n        \n        # The background is in the 0th position\n        self.classes = [_, 'without_mask','with_mask','mask_weared_incorrect']\n\n    def __getitem__(self, idx):\n\n        img_name = self.imgs[idx]\n        image_path = os.path.join(self.images_dir, img_name)\n\n        # changing the color and size of the image   \n        img = cv2.imread(image_path)\n        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)\n        img_res = cv2.resize(img_rgb, (self.width, self.height), cv2.INTER_AREA)\n        # image normalization\n        img_res /= 255.0\n        \n        # annotation\n        annot_filename = self.annotate[idx]\n        annot_file_path = os.path.join(self.annotation_dir, annot_filename)\n        \n        boxes = []\n        labels = []\n        tree = et.parse(annot_file_path)\n        root = tree.getroot()\n        \n        # image height and width\n        wt = img.shape[1]\n        ht = img.shape[0]\n        \n        # bounding boxes\n        for member in root.findall('object'):\n            labels.append(self.classes.index(member.find('name').text))\n            \n            # bounding box\n            xmin = int(member.find('bndbox').find('xmin').text)\n            xmax = int(member.find('bndbox').find('xmax').text)\n            \n            ymin = int(member.find('bndbox').find('ymin').text)\n            ymax = int(member.find('bndbox').find('ymax').text)\n            \n            \n            xmin_corr = (xmin/wt)*self.width\n            xmax_corr = (xmax/wt/1.003)*self.width\n            ymin_corr = (ymin/ht)*self.height\n            ymax_corr = (ymax/ht)*self.height\n            \n            boxes.append([xmin_corr, ymin_corr, xmax_corr, ymax_corr])\n        \n        # converting boxes to torch format.Tensor\n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n        \n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n\n        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)\n        \n        labels = torch.as_tensor(labels, dtype=torch.int64)\n\n\n        target = {}\n        target[\"boxes\"] = boxes\n        target[\"labels\"] = labels\n        target[\"area\"] = area\n        target[\"iscrowd\"] = iscrowd\n\n        image_id = torch.tensor([idx])\n        target[\"image_id\"] = image_id\n\n\n        if self.transforms:\n            \n            sample = self.transforms(image = img_res,\n                                     bboxes = target['boxes'],\n                                     labels = labels)\n            \n            img_res = sample['image']\n            target['boxes'] = torch.Tensor(sample['bboxes'])\n            \n            \n            \n        return img_res, target\n\n    def __len__(self):\n        return len(self.imgs)\n\n\n# checking the dataset\ndataset = FaceMaskDataset(images_dir, annotations_dir, 224, 224)\nprint('length of dataset = ', len(dataset), '\\n')\n\nimg, target = dataset[78]\nprint('Image shape = ', img.shape, '\\n','Target - ', target)","metadata":{"execution":{"iopub.status.busy":"2024-06-22T11:19:34.258185Z","iopub.execute_input":"2024-06-22T11:19:34.258860Z","iopub.status.idle":"2024-06-22T11:19:34.572335Z","shell.execute_reply.started":"2024-06-22T11:19:34.258829Z","shell.execute_reply":"2024-06-22T11:19:34.571371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Auxiliary function for bbox visualization\n\ndef plot_img_bbox(img, target):\n    fig, ax = plt.subplots(figsize=(5,5), ncols=1)\n    ax.imshow(img)\n    ax.axis('scaled')\n    for box in (target['boxes']):\n        x, y, width, height = box[0], box[1], box[2]-box[0], box[3]-box[1]\n        rect = patches.Rectangle((x, y),\n                                 width, height,\n                                 linewidth = 2,\n                                 edgecolor = 'r',\n                                 facecolor = 'none')\n\n        # Let's draw the bbox on top of the image\n        ax.add_patch(rect)\n    plt.show()\n    \nimg, target = dataset[35]\nplot_img_bbox(img, target)","metadata":{"execution":{"iopub.status.busy":"2024-06-22T11:19:34.573734Z","iopub.execute_input":"2024-06-22T11:19:34.574561Z","iopub.status.idle":"2024-06-22T11:19:35.024693Z","shell.execute_reply.started":"2024-06-22T11:19:34.574513Z","shell.execute_reply":"2024-06-22T11:19:35.023742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_transform(train):\n    \n    if train:\n        return A.Compose([\n# A.HorizontalFlip(0.5),\n# A.RandomBrightnessContrast(p=0.2),\n# A.ShiftScaleRotate(shift_limit=0.2, scale_limit=0.2, rotate_limit=30, p=0.5),\n                            ToTensorV2(p=1.0) \n                        ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n    else:\n        return A.Compose([\n                            ToTensorV2(p=1.0)\n                        ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})","metadata":{"execution":{"iopub.status.busy":"2024-06-22T11:19:35.025820Z","iopub.execute_input":"2024-06-22T11:19:35.026115Z","iopub.status.idle":"2024-06-22T11:19:35.031994Z","shell.execute_reply.started":"2024-06-22T11:19:35.026091Z","shell.execute_reply":"2024-06-22T11:19:35.031092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def collate_fn(batch):\n    return tuple(zip(*batch))","metadata":{"execution":{"iopub.status.busy":"2024-06-22T11:19:35.033362Z","iopub.execute_input":"2024-06-22T11:19:35.033950Z","iopub.status.idle":"2024-06-22T11:19:35.041521Z","shell.execute_reply.started":"2024-06-22T11:19:35.033915Z","shell.execute_reply":"2024-06-22T11:19:35.040573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = FaceMaskDataset(images_dir, annotations_dir, 600, 600, transforms= get_transform(train=True))\ndataset_test = FaceMaskDataset(images_dir, annotations_dir, 600, 600, transforms= get_transform(train=False))\n\n# let's divide the data into train and test\ntorch.manual_seed(1)\nindices = torch.randperm(len(dataset)).tolist()\n\ntest_split = 0.2\ntsize = int(len(dataset)*test_split)\ndataset = torch.utils.data.Subset(dataset, indices[:-tsize])\ndataset_test = torch.utils.data.Subset(dataset_test, indices[-tsize:])\n\ndata_loader = torch.utils.data.DataLoader(\n    dataset, batch_size=10, shuffle=True, num_workers=2,\n    collate_fn=collate_fn)\n\ndata_loader_test = torch.utils.data.DataLoader(\n    dataset_test, batch_size=10, shuffle=False, num_workers=2,\n    collate_fn=collate_fn)","metadata":{"execution":{"iopub.status.busy":"2024-06-22T11:19:35.042866Z","iopub.execute_input":"2024-06-22T11:19:35.043543Z","iopub.status.idle":"2024-06-22T11:19:35.060133Z","shell.execute_reply.started":"2024-06-22T11:19:35.043502Z","shell.execute_reply":"2024-06-22T11:19:35.059250Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Training","metadata":{}},{"cell_type":"code","source":"# We take the most advanced model from the FasterRCNN family, FASTERRCNN_RESET 50_FPS_V2\n\ndef get_model_instance(num_classes):\n    \n    model = torchvision.models.detection.fasterrcnn_resnet50_fpn_v2(weights='COCO_V1')\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-06-22T11:19:35.061498Z","iopub.execute_input":"2024-06-22T11:19:35.061788Z","iopub.status.idle":"2024-06-22T11:19:35.067115Z","shell.execute_reply.started":"2024-06-22T11:19:35.061764Z","shell.execute_reply":"2024-06-22T11:19:35.066093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\nnum_classes = 4\nnum_epochs = 40\nfrcnn_df = pd.DataFrame()\n\nmodel = get_model_instance(num_classes)\nmodel.to(device)\n\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=0.005,\n                                momentum=0.9, weight_decay=0.0005)\n\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n                                               step_size=3,\n                                               gamma=0.1)\nlen_dataloader = len(data_loader)\n\nstart_time = time.time()\n\nfor epoch in range(num_epochs):\n    model.train()  # Training model\n    epoch_loss = 0\n    for imgs, annotations in data_loader:\n        imgs = list(img.to(device) for img in imgs)\n        annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n        loss_dict = model(imgs, annotations)\n        losses = sum(loss for loss in loss_dict.values())\n        optimizer.zero_grad()\n        losses.backward()\n        optimizer.step()         \n        epoch_loss += losses.item()\n    print(f'Epoch: {epoch}, total loss: {epoch_loss}')\n    \n    model.eval()  # Validating model\n    metric = MeanAveragePrecision(box_format=\"xyxy\", class_metrics=False)\n    with torch.no_grad():\n        for imgs, annotations in data_loader_test:\n            imgs = list(img.to(device) for img in imgs)\n            annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n            preds = model(imgs)\n            metric.update(preds, annotations)\n    metric_res = metric.compute()\n    metric_res = {k: v.detach().numpy() for k, v in metric_res.items()}\n    metric_res['classes'] = ['all']\n    metric_res['epoch'] = [epoch]\n    frcnn_df = pd.concat([frcnn_df, pd.DataFrame(metric_res)])\n\nduration = time.time() - start_time\nprint('Training time: {:.0f}m {:.0f}s'.format(duration // 60, duration % 60))","metadata":{"execution":{"iopub.status.busy":"2024-06-22T11:19:35.068349Z","iopub.execute_input":"2024-06-22T11:19:35.068642Z","iopub.status.idle":"2024-06-22T12:33:59.993172Z","shell.execute_reply.started":"2024-06-22T11:19:35.068618Z","shell.execute_reply":"2024-06-22T12:33:59.991831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"frcnn_df","metadata":{"execution":{"iopub.status.busy":"2024-06-22T12:33:59.995013Z","iopub.execute_input":"2024-06-22T12:33:59.995963Z","iopub.status.idle":"2024-06-22T12:34:00.061458Z","shell.execute_reply.started":"2024-06-22T12:33:59.995929Z","shell.execute_reply":"2024-06-22T12:34:00.060517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"frcnn_df.plot(x='epoch', y='map_50', kind='line', title='FasterRCNNv2 mAP_50')","metadata":{"execution":{"iopub.status.busy":"2024-06-22T12:34:00.066104Z","iopub.execute_input":"2024-06-22T12:34:00.066457Z","iopub.status.idle":"2024-06-22T12:34:00.412096Z","shell.execute_reply.started":"2024-06-22T12:34:00.066431Z","shell.execute_reply":"2024-06-22T12:34:00.411167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The auxiliary function accepts the initial forecast and the threshold of you\n\ndef apply_nms(orig_prediction, iou_thresh=0.3):\n    \n    keep = torchvision.ops.nms(orig_prediction['boxes'], orig_prediction['scores'], iou_thresh)\n    \n    final_prediction = orig_prediction\n    final_prediction['boxes'] = final_prediction['boxes'][keep]\n    final_prediction['scores'] = final_prediction['scores'][keep]\n    final_prediction['labels'] = final_prediction['labels'][keep]\n    \n    return final_prediction\n\ndef torch_to_pil(img):\n    return transforms.ToPILImage()(img).convert('RGB')","metadata":{"execution":{"iopub.status.busy":"2024-06-22T12:34:00.413629Z","iopub.execute_input":"2024-06-22T12:34:00.414097Z","iopub.status.idle":"2024-06-22T12:34:00.420853Z","shell.execute_reply.started":"2024-06-22T12:34:00.414063Z","shell.execute_reply":"2024-06-22T12:34:00.419841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img, target = dataset_test[4]\n\nmodel.eval()\nwith torch.no_grad():\n    prediction = model([img.to(device)])[0]\n    \nprint('predicted #boxes: ', len(prediction['labels']))\nprint('real #boxes: ', len(target['labels']))","metadata":{"execution":{"iopub.status.busy":"2024-06-22T12:34:00.422012Z","iopub.execute_input":"2024-06-22T12:34:00.422324Z","iopub.status.idle":"2024-06-22T12:34:00.587026Z","shell.execute_reply.started":"2024-06-22T12:34:00.422292Z","shell.execute_reply":"2024-06-22T12:34:00.586090Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('EXPECTED OUTPUT')\nplot_img_bbox(torch_to_pil(img), target)","metadata":{"execution":{"iopub.status.busy":"2024-06-22T12:34:00.588295Z","iopub.execute_input":"2024-06-22T12:34:00.589193Z","iopub.status.idle":"2024-06-22T12:34:00.942992Z","shell.execute_reply.started":"2024-06-22T12:34:00.589166Z","shell.execute_reply":"2024-06-22T12:34:00.942032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('MODEL OUTPUT')\nplot_img_bbox(torch_to_pil(img), {k: v.to('cpu') for k, v in prediction.items()})","metadata":{"execution":{"iopub.status.busy":"2024-06-22T12:34:00.944219Z","iopub.execute_input":"2024-06-22T12:34:00.944523Z","iopub.status.idle":"2024-06-22T12:34:01.228585Z","shell.execute_reply.started":"2024-06-22T12:34:00.944497Z","shell.execute_reply":"2024-06-22T12:34:01.227538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nms_prediction = apply_nms(prediction, iou_thresh=0.2)\nprint('NMS APPLIED MODEL OUTPUT')\nplot_img_bbox(torch_to_pil(img), {k: v.to('cpu') for k, v in nms_prediction.items()})","metadata":{"execution":{"iopub.status.busy":"2024-06-22T12:34:01.229918Z","iopub.execute_input":"2024-06-22T12:34:01.230259Z","iopub.status.idle":"2024-06-22T12:34:01.516506Z","shell.execute_reply.started":"2024-06-22T12:34:01.230231Z","shell.execute_reply":"2024-06-22T12:34:01.515549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualization of the results","metadata":{}},{"cell_type":"code","source":"def plot_image(img_tensor, annotation, predict=True):\n    \n    fig, ax = plt.subplots(1)\n    fig.set_size_inches(12.5, 6.5)\n    img = img_tensor.cpu().data\n    mask_dic = {1:'without_mask', 2:'with_mask', 3:'mask_weared_incorrect'}\n    color_dict = {'without_mask': 'r', 'with_mask': 'g', 'mask_weared_incorrect': 'y'}\n\n    ax.imshow(img.permute(1, 2, 0))\n    ax.set_title('Prediction' if predict else 'Target')\n    \n    annotation = {k: v.detach().to('cpu') for k, v in annotation.items()}\n    \n    for i,box in enumerate(annotation[\"boxes\"]):\n        \n        xmin, ymin, xmax, ymax = box\n        label = mask_dic[int(annotation['labels'][i].data)]\n\n        rect = patches.Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),linewidth=1,edgecolor=color_dict[label],facecolor='none')\n\n        ax.add_patch(rect)\n        \n        if predict:\n            score = int((annotation['scores'][i].data) * 100)\n            ax.text(xmin, ymin, f\"{score}%\", horizontalalignment='center', verticalalignment='center',fontsize=20,color=color_dict[label])\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-22T12:34:01.517664Z","iopub.execute_input":"2024-06-22T12:34:01.517991Z","iopub.status.idle":"2024-06-22T12:34:01.527540Z","shell.execute_reply.started":"2024-06-22T12:34:01.517964Z","shell.execute_reply":"2024-06-22T12:34:01.526682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for imgs, annotations in data_loader:\n        imgs = list(img.to(device) for img in imgs)\n        annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n        break","metadata":{"execution":{"iopub.status.busy":"2024-06-22T12:34:01.528587Z","iopub.execute_input":"2024-06-22T12:34:01.528848Z","iopub.status.idle":"2024-06-22T12:34:02.096844Z","shell.execute_reply.started":"2024-06-22T12:34:01.528821Z","shell.execute_reply":"2024-06-22T12:34:02.095745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with torch.no_grad():\n    preds = model(imgs)","metadata":{"execution":{"iopub.status.busy":"2024-06-22T12:34:02.098528Z","iopub.execute_input":"2024-06-22T12:34:02.098845Z","iopub.status.idle":"2024-06-22T12:34:02.773007Z","shell.execute_reply.started":"2024-06-22T12:34:02.098817Z","shell.execute_reply":"2024-06-22T12:34:02.772161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nms_prediction = apply_nms(preds[4], iou_thresh=0.2)\nplot_image(imgs[4], nms_prediction)\nplot_image(imgs[4].to('cpu'), annotations[4], False)","metadata":{"execution":{"iopub.status.busy":"2024-06-22T12:34:02.774374Z","iopub.execute_input":"2024-06-22T12:34:02.774813Z","iopub.status.idle":"2024-06-22T12:34:03.762529Z","shell.execute_reply.started":"2024-06-22T12:34:02.774781Z","shell.execute_reply":"2024-06-22T12:34:03.761540Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nms_prediction = apply_nms(preds[7], iou_thresh=0.2)\nplot_image(imgs[7], nms_prediction)\nplot_image(imgs[7].to('cpu'), annotations[7],False)","metadata":{"execution":{"iopub.status.busy":"2024-06-22T12:34:03.763820Z","iopub.execute_input":"2024-06-22T12:34:03.764145Z","iopub.status.idle":"2024-06-22T12:34:04.773935Z","shell.execute_reply.started":"2024-06-22T12:34:03.764120Z","shell.execute_reply":"2024-06-22T12:34:04.772687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nms_prediction = apply_nms(preds[0], iou_thresh=0.2)\nplot_image(imgs[0], nms_prediction)\nplot_image(imgs[0].to('cpu'), annotations[0],False)","metadata":{"execution":{"iopub.status.busy":"2024-06-22T12:34:04.776505Z","iopub.execute_input":"2024-06-22T12:34:04.777173Z","iopub.status.idle":"2024-06-22T12:34:06.018784Z","shell.execute_reply.started":"2024-06-22T12:34:04.777137Z","shell.execute_reply":"2024-06-22T12:34:06.017925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nms_prediction = apply_nms(preds[2], iou_thresh=0.2)\nplot_image(imgs[2], nms_prediction)\nplot_image(imgs[2].to('cpu'), annotations[2],False)","metadata":{"execution":{"iopub.status.busy":"2024-06-22T12:34:06.020166Z","iopub.execute_input":"2024-06-22T12:34:06.020560Z","iopub.status.idle":"2024-06-22T12:34:06.879581Z","shell.execute_reply.started":"2024-06-22T12:34:06.020530Z","shell.execute_reply":"2024-06-22T12:34:06.878756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# YOLOv8","metadata":{}},{"cell_type":"markdown","source":"For this assignment from the YOLO family, we will use the YOLOv8 model as the most modern and advanced of the existing ones.","metadata":{}},{"cell_type":"markdown","source":"# Creating a dataset","metadata":{}},{"cell_type":"code","source":"def xml_to_yolo(xml_path):\n    \"\"\"\n    A function for reading an annotation file in XML format with\n    subsequent conversion to YOLO format. As a\n    The input parameter takes the path to the file.\n    Returns the data prepared for writing to the file.\n    \"\"\"\n    bboxes = []\n    label_to_id = {'without_mask': 0,'with_mask': 1,'mask_weared_incorrect': 2}\n    tree = et.parse(xml_path)\n    root = tree.getroot()\n    size = root.find('size')\n    w = int(size.find('width').text)\n    h = int(size.find('height').text)\n    # We go through all the objects in the annotation file\n    for obj in root.findall('object'):\n        label = obj.find('name').text\n        bndbox_tree = obj.find('bndbox')\n        x_min = int(bndbox_tree.find('xmin').text)\n        y_min = int(bndbox_tree.find('ymin').text)\n        x_max = int(bndbox_tree.find('xmax').text)\n        y_max = int(bndbox_tree.find('ymax').text)\n        \n        # Converting coordinates to YOLO format\n        x_center = (x_max + x_min) / (2 * w)\n        y_center = (y_max + y_min) / (2 * h)\n        width = (x_max - x_min) / w\n        height = (y_max - y_min) / h\n        bboxes.append('{} {:.3f} {:.3f} {:.3f} {:.3f}'.format(label_to_id[label], x_center, y_center, width, height))\n    return bboxes\n\n\ndef create_yolo_dataset(input_path, save_path, images, data_type):\n    \"\"\"\n    A function for creating a dataset for the YOLO model with conversion\n    annotations from the PASCAL VOC format.\n    Input parameters:\n    input_path - path to the directory with image and annotation files\n    save_path is the path to the directory of the dataset being created\n    images - list of image files for the dataset\n    data_type - data type (train/val)\n    \"\"\"\n\n    img_dest_dir = os.path.join(save_path, 'images', data_type)\n    lbl_dest_dir = os.path.join(save_path, 'labels', data_type)\n    os.makedirs(img_dest_dir)\n    os.makedirs(lbl_dest_dir)\n    for img in images:\n        shutil.copy(os.path.join(input_path, 'images', img), img_dest_dir)\n        ann_path = os.path.join(input_path, 'annotations', img.replace('png', 'xml'))\n        bboxes = xml_to_yolo(ann_path)\n        lbl_path = os.path.join(lbl_dest_dir, img.replace('png', 'txt'))\n        with open(lbl_path, 'w') as f:\n            f.write('\\n'.join(bboxes))","metadata":{"execution":{"iopub.status.busy":"2024-06-22T12:34:06.880863Z","iopub.execute_input":"2024-06-22T12:34:06.881429Z","iopub.status.idle":"2024-06-22T12:34:06.894150Z","shell.execute_reply.started":"2024-06-22T12:34:06.881402Z","shell.execute_reply":"2024-06-22T12:34:06.893291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's create a dataset for the YOLO model\nsource_dir = '/kaggle/input/face-mask-detection'\nimages_dir = os.path.join(source_dir, 'images')\ndest_dir = 'datasets/facemask'\n\nimgs = [img for img in sorted(os.listdir(images_dir))]\n\nimgs_train, imgs_val = train_test_split(imgs, test_size=0.2, random_state=1)\n\ncreate_yolo_dataset(source_dir, dest_dir, imgs_train, data_type='train')\ncreate_yolo_dataset(source_dir, dest_dir, imgs_val, data_type='val')\n\n# Let's create a configuration file for the dataset\nfacemask_yaml = f\"\"\"\n    path: facemask\n    train: images/train\n    val: images/val\n    names:\n        0: without_mask\n        1: with_mask\n        2: mask_weared_incorrect\n    \"\"\"\n\nwith open('facemask.yaml', 'w') as f:\n    f.write(facemask_yaml)","metadata":{"execution":{"iopub.status.busy":"2024-06-22T12:34:06.895360Z","iopub.execute_input":"2024-06-22T12:34:06.895697Z","iopub.status.idle":"2024-06-22T12:34:08.508182Z","shell.execute_reply.started":"2024-06-22T12:34:06.895666Z","shell.execute_reply":"2024-06-22T12:34:08.507360Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Training YOLOv8","metadata":{}},{"cell_type":"code","source":"settings.update({'wandb': False})  # Disabling logging of Weights & Biases\n\nmodel = YOLO(\"yolov8s.pt\")\nstart_time = time.time()\nmodel.train(data=\"facemask.yaml\", epochs=num_epochs, batch=16, imgsz=600, cache=True, val=True, exist_ok=True, verbose = False)\nduration = time.time() - start_time\nprint('Training time: {:.0f}m {:.0f}s'.format(duration // 60, duration % 60))","metadata":{"execution":{"iopub.status.busy":"2024-06-22T12:34:08.509392Z","iopub.execute_input":"2024-06-22T12:34:08.510023Z","iopub.status.idle":"2024-06-22T12:44:19.191047Z","shell.execute_reply.started":"2024-06-22T12:34:08.509985Z","shell.execute_reply":"2024-06-22T12:44:19.189821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Визуализация результатов","metadata":{}},{"cell_type":"code","source":"# Метрики обучения\nyolo_df = pd.read_csv('/kaggle/working/runs/detect/train/results.csv', sep=',\\s+', engine='python')\nyolo_df","metadata":{"execution":{"iopub.status.busy":"2024-06-22T12:44:19.192834Z","iopub.execute_input":"2024-06-22T12:44:19.193471Z","iopub.status.idle":"2024-06-22T12:44:19.238133Z","shell.execute_reply.started":"2024-06-22T12:44:19.193441Z","shell.execute_reply":"2024-06-22T12:44:19.237164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Butch with marked up data\nimg = Image.open('/kaggle/working/runs/detect/train/val_batch0_labels.jpg')\ndisplay(img)","metadata":{"execution":{"iopub.status.busy":"2024-06-22T12:44:19.239585Z","iopub.execute_input":"2024-06-22T12:44:19.240324Z","iopub.status.idle":"2024-06-22T12:44:20.052281Z","shell.execute_reply.started":"2024-06-22T12:44:19.240289Z","shell.execute_reply":"2024-06-22T12:44:20.050838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Butch with predictions\nimg = Image.open('/kaggle/working/runs/detect/train/val_batch0_pred.jpg')\ndisplay(img)","metadata":{"execution":{"iopub.status.busy":"2024-06-22T12:44:20.053667Z","iopub.execute_input":"2024-06-22T12:44:20.054239Z","iopub.status.idle":"2024-06-22T12:44:20.839367Z","shell.execute_reply.started":"2024-06-22T12:44:20.054209Z","shell.execute_reply":"2024-06-22T12:44:20.838411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Prediction by a trained model №1\nim1 = Image.open('/kaggle/input/face-mask-detection/images/maksssksksss272.png')\nresults = model.predict(source=im1, iou=0.2)\nres_plotted = results[0].plot()\nplt.imshow(cv2.cvtColor(res_plotted, cv2.COLOR_BGR2RGB))\nplt.title(\"Prediction\")\nplt.axis(False)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-22T12:44:20.840605Z","iopub.execute_input":"2024-06-22T12:44:20.841146Z","iopub.status.idle":"2024-06-22T12:44:21.328154Z","shell.execute_reply.started":"2024-06-22T12:44:20.841097Z","shell.execute_reply":"2024-06-22T12:44:21.327218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Prediction by a trained model №2\nim2 = Image.open('/kaggle/input/face-mask-detection/images/maksssksksss111.png')\nresults = model.predict(source=im2, iou=0.2)\nres_plotted = results[0].plot()\nplt.imshow(cv2.cvtColor(res_plotted, cv2.COLOR_BGR2RGB))\nplt.title(\"Prediction\")\nplt.axis(False)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-22T12:44:21.329580Z","iopub.execute_input":"2024-06-22T12:44:21.330257Z","iopub.status.idle":"2024-06-22T12:44:21.611794Z","shell.execute_reply.started":"2024-06-22T12:44:21.330222Z","shell.execute_reply":"2024-06-22T12:44:21.610899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Prediction by a trained model №3\nim3 = Image.open('/kaggle/input/face-mask-detection/images/maksssksksss394.png')\nresults = model.predict(source=im3, iou=0.2)\nres_plotted = results[0].plot()\nplt.imshow(cv2.cvtColor(res_plotted, cv2.COLOR_BGR2RGB))\nplt.title(\"Prediction\")\nplt.axis(False)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-22T12:44:21.613189Z","iopub.execute_input":"2024-06-22T12:44:21.613826Z","iopub.status.idle":"2024-06-22T12:44:21.910417Z","shell.execute_reply.started":"2024-06-22T12:44:21.613788Z","shell.execute_reply":"2024-06-22T12:44:21.909499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Prediction by a trained model №4\nim4 = Image.open('/kaggle/input/face-mask-detection/images/maksssksksss571.png')\nresults = model.predict(source=im4, iou=0.2)\nres_plotted = results[0].plot()\nplt.imshow(cv2.cvtColor(res_plotted, cv2.COLOR_BGR2RGB))\nplt.title(\"Prediction\")\nplt.axis(False)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-22T12:44:21.911653Z","iopub.execute_input":"2024-06-22T12:44:21.911982Z","iopub.status.idle":"2024-06-22T12:44:22.238826Z","shell.execute_reply.started":"2024-06-22T12:44:21.911954Z","shell.execute_reply":"2024-06-22T12:44:22.237918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Prediction by a trained model №5\nim5 = Image.open('//kaggle/input/face-mask-detection/images/maksssksksss826.png')\nresults = model.predict(source=im5, iou=0.2)\nres_plotted = results[0].plot()\nplt.imshow(cv2.cvtColor(res_plotted, cv2.COLOR_BGR2RGB))\nplt.title(\"Prediction\")\nplt.axis(False)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-22T12:44:22.240250Z","iopub.execute_input":"2024-06-22T12:44:22.240631Z","iopub.status.idle":"2024-06-22T12:44:22.535742Z","shell.execute_reply.started":"2024-06-22T12:44:22.240599Z","shell.execute_reply":"2024-06-22T12:44:22.534817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Comparing the mAP_50 metric for both models\n\nyolo_df = pd.read_csv('/kaggle/working/runs/detect/train/results.csv', sep=',\\s+', engine='python')\n\nfig, ax = plt.subplots(figsize=(8, 5))\nax.plot(frcnn_df['epoch'], frcnn_df['map_50'], label='FasterRCNNv2')\nax.plot(yolo_df['epoch'], yolo_df['metrics/mAP50(B)'], label='YOLOv8')\nax.set_xlabel('Epoch')\nax.set_ylabel('mAP_50')\nax.legend()\nax.set_title('YOLOv8 vs FasterRCNNv2 on validation dataset')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-22T12:44:22.537107Z","iopub.execute_input":"2024-06-22T12:44:22.537895Z","iopub.status.idle":"2024-06-22T12:44:22.848796Z","shell.execute_reply.started":"2024-06-22T12:44:22.537843Z","shell.execute_reply":"2024-06-22T12:44:22.847825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Result","metadata":{}},{"cell_type":"code","source":"# The best mAP_50 values for each of the models\nprint('FasterRCNNv2 mAP_50 max: {:.3f}'.format(frcnn_df['map_50'].max()))\nprint('YOLOv8 mAP_50 max: {:.3f}'.format(yolo_df['metrics/mAP50(B)'].max()))","metadata":{"execution":{"iopub.status.busy":"2024-06-22T12:44:22.849912Z","iopub.execute_input":"2024-06-22T12:44:22.850192Z","iopub.status.idle":"2024-06-22T12:44:22.855708Z","shell.execute_reply.started":"2024-06-22T12:44:22.850168Z","shell.execute_reply":"2024-06-22T12:44:22.854682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"(English)\nFrom the data obtained, it can be seen that by all criteria in our case, YOLOv8 showed better results than the faster R-CNN. The YOLO model works faster than the faster RCN, is more accurate and shows better results on the mAP_50 metric, the difference is quite significant (by 0.47).\nThe faster RCN is usually seen as a slower but more accurate model. In this case, everything is different. The reason may be the imbalance of the data and the fact that YOLO, starting from version 7, includes a loss of focus function that takes into account the imbalance of classes, and in version 8 the loss function is changed to improve accuracy. Apparently, this approach works better than the weighted loading implemented here to speed up R-CNN.\nThe training was conducted on GPU P100 power.","metadata":{}},{"cell_type":"markdown","source":"(На Русском)\nИз полученных данных видно, что по всем критериям в нашем случае YOLOv8 показал лучшие результаты, чем более быстрый R-CNN. Модель YOLO работает быстрее, чем более быстрый R-CNN, более точна и показывает лучшие результаты по метрике mAP_50, разница довольно существенная(на 0.47).\nБолее быстрая R-CNN обычно рассматривается как более медленная, но более точная модель. В данном случае все по-другому. Причина, возможно, в несбалансированности данных и в том факте, что YOLO, начиная с версии 7, включает функцию потери фокуса, которая учитывает дисбаланс классов, а в версии 8 функция потери изменена для повышения точности. По-видимому, этот подход работает лучше, чем взвешенная загрузка, реализованная здесь для ускорения R-CNN.\nОбучение проводилось на мощности GPU P100.","metadata":{}}]}